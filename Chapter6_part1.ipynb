{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2d6e30",
   "metadata": {},
   "source": [
    "# **Chapter 6. Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4301ad-93fc-4ab2-bd36-e0e18554a40c",
   "metadata": {},
   "source": [
    "## **6.1. Introduction to Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579219b-fb19-4fc3-b7f9-ebf14ff8fa02",
   "metadata": {},
   "source": [
    "**Machine Learning (ML)** is a subset of artificial intelligence (AI) focused on building systems that learn from and make predictions or decisions based on data. ML algorithms improve their performance as the amount of data available for learning increases.\n",
    "\n",
    "Machine learning has emerged as a transformative tool in chemistry, offering new approaches to solving complex chemical problems. By leveraging patterns in data, ML algorithms can predict molecular properties, optimize chemical reactions, design new materials, and more, often with greater efficiency than traditional methods.\n",
    "\n",
    "**Types of Machine Learning:**\n",
    "\n",
    "- **Supervised Learning:** Models are trained on labeled data, learning to predict outputs from inputs.\n",
    "- **Unsupervised Learning:** Models identify patterns in data without any labels.\n",
    "- **Reinforcement Learning:** Models learn to make sequences of decisions by receiving feedback on their actions.\n",
    "\n",
    "**Applications of Machine Learning in Chemistry:**\n",
    "\n",
    "- **Predictive Modeling:** Using ML to predict properties of molecules such as solubility, toxicity, reactivity, or the outcomes of chemical reactions such as yield, chemoselectivity and regioselectivity.\n",
    "- **Reaction Optimization:** Finding optimal conditions for chemical reactions.\n",
    "- **Quantitative Analysis:** Using ML to perform quantitative structure-activity relationships (QSAR) and quantitative structure-property relationships (QSPR).\n",
    "- **Drug Discovery:** Virtual screening, ADMET prediction.\n",
    "- **Molecular Dynamics and Simulations:** Applying ML to improve the efficiency of molecular dynamics simulations.\n",
    "- **Material Design:** Discovering new materials with desired properties.\n",
    "\n",
    "**Challenges and Limitations**\n",
    "- **Generalization and Overfitting**\n",
    "    - The balance between model complexity and predictive power.\n",
    "    - Methods to prevent overfitting such as cross-validation.\n",
    "- **Interpretability**\n",
    "    - The \"black box\" nature of some ML models and the importance of model interpretability in chemistry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a304a2-f0a1-47c6-be6c-21cbb3ac5835",
   "metadata": {},
   "source": [
    "## **6.2. Data Preprocessing for Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf9325-e9b5-461f-a20d-adb342da4794",
   "metadata": {},
   "source": [
    "Preprocessing is crucial in transforming raw data into a format suitable for machine learning. It involves cleaning data, handling missing values, normalization, feature extraction, and data transformation. Preprocessed data leads to more efficient training of machine learning models, can enhance model accuracy, and helps in achieving more reliable predictions.\n",
    "\n",
    "Preprocessing steps include:\n",
    "- **Cleaning Data:** Removing duplicates, handling missing values, or filtering out irrelevant data.\n",
    "- **Feature Extraction:** Transforming chemical compounds into a suitable format for machine learning, such as SMILES (Simplified Molecular Input Line Entry System) strings, molecular fingerprints, or graph representations.\n",
    "- **Normalization/Standardization:** Scaling data to a specific range or distribution. This is important for methods sensitive to the scale of input data.\n",
    "- **Encoding Categorical Data** Transform categorical data into numeric data.\n",
    "- **Data Transformation:** Converting data into formats that can be efficiently processed by ML algorithms, such as one-hot encoding for categorical data.\n",
    "- **Data Augmentation:** Generating additional data points from existing data.\n",
    "- **Dimensionality Reduction:** High-dimensional data, often encountered in chemistry, can lead to issues like overfitting and long training times. Dimensionality reduction techniques, such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE), reduce the number of features while retaining most of the important information. This step is crucial for simplifying models, improving interpretability, and sometimes enhancing model performance.\n",
    "- **Splitting Data:** Splitting the data into smaller datasets for training, evaluation, and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601979fe-7c5b-4678-8c9d-392c575228cf",
   "metadata": {},
   "source": [
    "### **6.2.1. Normalization and Standardization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477aee8-3168-485a-bff8-65e70612a705",
   "metadata": {},
   "source": [
    "In machine learning, normalization and standardization are essential preprocessing techniques that ensure the scalability and effectiveness of algorithms that are sensitive to the scale of input data. They help improve the model training process by transforming features to have a similar range or distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c34fc7-7f6e-4c0f-b9ac-87c11cfea488",
   "metadata": {},
   "source": [
    "#### **6.2.1.1. Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ce3a5-cabc-447f-90ad-8fa057f2a95f",
   "metadata": {},
   "source": [
    "Normalization typically refers to the process of scaling down the values of a feature to a specific range, typically [0, 1]. This is done to ensure that the scale of different features does not disproportionately influence the results of the model. The method used for normalization is often called Min-Max Scaling.\n",
    "\n",
    "The Min-Max scaling formula is given by:\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "where $X$ is the original value, $X_{min}$ is the minimum value of the feature, and $X_{max}$ is the maximum value of the feature.\n",
    "\n",
    "**Example:** Min-Max Scaler using scikit-learn\n",
    "\n",
    "Here’s how to apply Min-Max normalization using the `MinMaxScaler` from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ece09-1d78-4841-99ae-f3b655543594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2],\n",
    "                 [3, 4],\n",
    "                 [5, 6]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Normalized Data:\\n\", normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baca7b7-1461-4586-9258-5c3abb844b10",
   "metadata": {},
   "source": [
    "#### **6.2.1.2. Standardization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048f405-1cb4-4a94-9712-d0d3a6d78abd",
   "metadata": {},
   "source": [
    "Standardization, on the other hand, refers to scaling the features such that they have a mean of 0 and a standard deviation of 1. This is particularly useful in algorithms that assume data is normally distributed. The standard score \\(Z\\) is calculated using the formula:\n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $X$ is the original value, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "**Example:** Standard Scaler using scikit-learn\n",
    "\n",
    "Here’s how to apply standardization using the `StandardScaler` from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456b17c-f8b8-43ff-a938-b70ba5f107b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2],\n",
    "                 [3, 4],\n",
    "                 [5, 6]])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Standardized Data:\\n\", standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8021d-b879-46d9-9327-b83415d43a2d",
   "metadata": {},
   "source": [
    "<p style=\"background-color: lightgreen; text-align: center; font-size: 18px; color: red; padding: 5px; border-radius: 10px;\"><b>Exercise 1</b></p>\n",
    "\n",
    "1. **Generate Sample Data:**\n",
    "    - Create a 1D NumPy array consisting of 10 random values between 1 and 100.\n",
    "\n",
    "2. **Perform Min-Max Scaling:**\n",
    "    - Use `MinMaxScaler` from `sklearn.preprocessing` to normalize this data to the range [0, 1].\n",
    "    - Print the original data and the normalized data.\n",
    "\n",
    "3. **Transform New Data and Inverse Transform:**\n",
    "    - Create a new 1D NumPy array consisting of 5 random values between 1 and 100. Transform the data using the `MinMaxScaler`, then inverse transform the result.\n",
    "\n",
    "4. **Perform Standardization:**\n",
    "    - Use `StandardScaler` from `sklearn.preprocessing` to standardize the original data.\n",
    "    - Print the original data and the standardized data.\n",
    "\n",
    "5. **Unscale the Standardized Data:**\n",
    "    - Create a new 1D NumPy array consisting of 5 random values between 1 and 100. Transform the data using the `MinMaxScaler`, then inverse transform the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0e6db-db57-475b-822a-9cb1f2d637eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaf34a-0049-42f5-a5bd-a55ee153aeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d69ff5-6311-40cb-86ba-4055dbc8174c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6bde1f-dcaf-48f6-99d8-15f61732e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa4817-e49d-4ddc-92e5-a0b81e627015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2fbdd9f-f8f7-46e1-9852-e29635c0b850",
   "metadata": {},
   "source": [
    "### **6.2.2. Encoding Categorical Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad344c9-2b94-4cbe-8c08-eee554ae23ef",
   "metadata": {},
   "source": [
    "In machine learning, it is common to encounter categorical variables that need to be encoded into numerical values so that algorithms can process them effectively. Encoding categorical data is essential for ensuring that models can interpret the variables correctly and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f20f4-c130-4a81-ac17-4f6fd8dbd15d",
   "metadata": {},
   "source": [
    "#### ***6.2.2.1. Label Encoding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361d4c9-a159-4253-8bf3-b615c9f27fe8",
   "metadata": {},
   "source": [
    "Label encoding is another technique for converting categorical variables into numerical form. In this method, each unique category in the feature is assigned an integer value. While this method is simpler than one-hot encoding, it is important to note that it introduces an ordinal relationship between categories, which may not always be appropriate.\n",
    "\n",
    "- **Process:** Assign an integer value to each unique category in the categorical variable.\n",
    "\n",
    "- **Application:** Label encoding is ideal for ordinal variables where there is a clear ordering among the categories (e.g., 'Low', 'Medium', 'High').\n",
    "\n",
    "**Example: Using Label Encoding with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbda1d-041a-457c-b735-efded5912069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = ['Red', 'Blue', 'Green', 'Blue', 'Red']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Applying Label Encoding\n",
    "encoded_labels = label_encoder.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nLabel Encoded Data:\\n\", encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413acc48-1441-482a-aa17-1bfbdd472a9d",
   "metadata": {},
   "source": [
    "#### ***6.2.2.2. One-Hot Encoding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdc455-277d-4964-9f52-1421be33f4f1",
   "metadata": {},
   "source": [
    "One-hot encoding is a popular method for converting categorical variables into a binary (0 or 1) format. Each category in the variable is represented as a separate binary feature. This technique avoids implying ordinality between categories, which is crucial when the categorical variable does not have a natural ordering.\n",
    "\n",
    "- **Process:**\n",
    "  1. Identify all unique categories in the categorical variable.\n",
    "  2. Create a new binary feature for each category.\n",
    "  3. Assign a 1 or 0 in the new features based on the original category.\n",
    "\n",
    "- **Application:** One-hot encoding is particularly useful for nominal variables, where there is no ordinal relationship between the categories.\n",
    "\n",
    "**Example: Using One-Hot Encoding with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cbc0a-7c03-452d-82d7-dacc5dfb30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([['Red'],\n",
    "                 ['Blue'],\n",
    "                 ['Green'],\n",
    "                 ['Blue'],\n",
    "                 ['Red']])\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Applying One-Hot Encoding\n",
    "one_hot_encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nOne-Hot Encoded Data:\\n\", one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49946220-c2cf-44b1-9547-2cd73a065638",
   "metadata": {},
   "source": [
    "### **6.2.3. Splitting Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98fab5-6bc8-4431-865f-0f86bd2e9e8e",
   "metadata": {},
   "source": [
    "In order to evaluate a model, we have to split the data into 2 sets: train set and test set. The train dataset is used for training the model, and the test dataset is used for evaluation.\n",
    "\n",
    "![Train-test split](images/train_test_split.png)\n",
    "\n",
    "In training models with multiple iterations (e.g. ANN models). The dataset is usually splitted into 3 sets: train set, validation set, and test set. The validation dataset is used for monitoring the training process of the model in order to avoid underfitting and overfitting during training.\n",
    "\n",
    "![Train-val-test split](images/train_val_test_split.png)\n",
    "\n",
    "**Example: Splitting data with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e336ff-313a-40e5-b23f-6a5fd9c79f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Dataset\n",
    "data = pd.read_csv('./datasets/IrisFlower.csv')\n",
    "\n",
    "# Explore the Data\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the inputs and output\n",
    "x = data.drop('Species', axis=1)\n",
    "y = data['Species']\n",
    "\n",
    "# Split the Data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(f'Shape of x_train: {x_train.shape}')\n",
    "print(f'Shape of x_test: {x_test.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f63e05-17d8-49fa-bd3d-f62ef1aeca47",
   "metadata": {},
   "source": [
    "<p style=\"background-color: lightgreen; text-align: center; font-size: 18px; color: red; padding: 5px; border-radius: 10px;\"><b>Exercise 2</b></p>\n",
    "\n",
    "1. **Load the Dataset:** Start by loading the breast cancer dataset from the `BreastCancer.csv` file using pandas.\n",
    "\n",
    "2. **Get the Input and Output Columns:** Use `diagnosis` as output and all other columns as inputs.\n",
    "\n",
    "3. **Scale the Data:** Apply min-max scaling for all the input features.\n",
    "\n",
    "3. **Encode the Output** Encode the output column: `0` for `B` (benign), `1` for `M` (malignant) using `LabelEncoder` and `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673c3c4-67f8-4331-9cbe-f32957f6f7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571f564-b25f-4232-b75f-0b8e39c24eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b1fc9-9c98-42fc-8581-fa4abbd27763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160169c8-ddb3-48ef-9b72-9ef5efc57d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52dc42e9-4ecf-4e5e-836c-e7e1fc219ce9",
   "metadata": {},
   "source": [
    "### **6.2.4. Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea98800-8c86-42ba-a5ad-2fe716f0b3b6",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a crucial preprocessing step in machine learning that aims to reduce the number of features in a dataset while retaining its essential characteristics and information. This reduction is particularly important in high-dimensional data, which can lead to issues such as overfitting, increased computation time, and difficulty in visualizing data.\n",
    "\n",
    "**Why Use Dimensionality Reduction?**\n",
    "\n",
    "1. **Improved Performance:** By reducing the number of features, models can train faster and run more efficiently, often resulting in improved performance metrics.\n",
    "2. **Visualization:** Lower-dimensional data can be visualized easily, enabling better understanding and insights.\n",
    "3. **Mitigation of Overfitting:** Fewer features can help prevent models from becoming overly complex and fitting noise in the training data.\n",
    "4. **Noise Reduction:** Removing less informative features can help reduce noise and improve model generalization.\n",
    "\n",
    "**Common Dimensionality Reduction Techniques**\n",
    "\n",
    "- **1. Principal Component Analysis (PCA)**\n",
    "- **2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec803767-4d90-4b68-a341-15e923646376",
   "metadata": {},
   "source": [
    "#### ***6.2.4.1. Principal Component Analysis (PCA)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169e6ac-ddef-4227-b20b-8d45a75f9153",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is one of the most widely used dimensionality reduction techniques. It transforms the data into a new coordinate system where the greatest variance by any projection lies on the first coordinate (the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "- **Process:**\n",
    "  1. Standardize the data.\n",
    "  2. Compute the covariance matrix.\n",
    "  3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "  4. Sort the eigenvalues and their corresponding eigenvectors.\n",
    "  5. Select a subset of the eigenvectors as principal components.\n",
    "  6. Transform the original data into the new feature subspace.\n",
    "\n",
    "- **Application:** PCA is commonly used for exploratory data analysis, image compression, and as a preprocessing step before feeding data into machine learning models.\n",
    "\n",
    "**Example: Using PCA with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db277af6-72fc-43cb-a084-92d1fd7d557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.random.rand(100, 5)  # 100 samples and 5 features\n",
    "print(\"Data Shape:\", data.shape)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(\"Reduced Data Shape:\", reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd4958-3779-4f82-bf40-5559b403abb1",
   "metadata": {},
   "source": [
    "#### ***6.2.4.2. t-Distributed Stochastic Neighbor Embedding (t-SNE)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ab8f4-58a8-4d0c-8889-eedf37dda9e6",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique particularly popular for visualizing high-dimensional datasets in lower dimensions (typically 2D or 3D).\n",
    "\n",
    "- **Characteristics:**\n",
    "  - t-SNE converts affinities of data points to probabilities.\n",
    "  - It seeks to minimize the divergence between two probability distributions: one for the high-dimensional space and one for the low-dimensional space.\n",
    "\n",
    "- **Application:** t-SNE is widely used for visualizing complex data structures, such as gene expression profiles or high-dimensional feature spaces in image data.\n",
    "\n",
    " **Example: Using t-SNE with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b073d97-11d6-4b54-a989-62c6935e9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample data\n",
    "data = np.random.rand(100, 50)  # 100 samples and 50 features\n",
    "print(\"Data Shape:\", data.shape)\n",
    "\n",
    "# Applying t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_reduced_data = tsne.fit_transform(data)\n",
    "\n",
    "print(\"t-SNE Reduced Data Shape:\", tsne_reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba01989-8bba-4a80-9cc4-1ae43de0d681",
   "metadata": {},
   "source": [
    "#### ***6.2.4.3. Linear Discriminant Analysis (LDA)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e96fdb-8cdf-4fdb-80ae-f28f30f4bded",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that is used when the data is labeled. It seeks to find a linear combination of features that characterizes or separates two or more classes.\n",
    "\n",
    "- **Characteristics:**\n",
    "  - LDA aims to maximize the distance between different classes and minimize the distance within the same class.\n",
    "\n",
    "- **Application:** LDA is often used in pattern recognition and machine learning as a way to reduce dimension while preserving as much class discriminatory information as possible.\n",
    "\n",
    "**Example: Using LDA with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bc8cc-267f-45f8-90de-dd9e01dbcfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Sample data with labels\n",
    "x = np.random.rand(100, 5)  # 100 samples and 5 features\n",
    "y = np.random.choice([0, 1], size=100)  # Binary labels\n",
    "print(\"Data Shape:\", x.shape)\n",
    "\n",
    "# Applying LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda_reduced_data = lda.fit_transform(x, y)\n",
    "\n",
    "print(\"LDA Reduced Data Shape:\", lda_reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbbabf4-1c36-4ac3-9115-cef5fa96e2be",
   "metadata": {},
   "source": [
    "#### ***6.2.4.4. Non-negative Matrix Factorization (NMF)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab16050-c8ad-426d-90d0-013c1bc282c7",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorization (NMF) is a group of algorithms in multivariate statistics and linear algebra where a matrix is factorized into two non-negative matrices. This approach is particularly useful when dealing with data that is inherently non-negative, like document-term matrices.\n",
    "\n",
    "- **Characteristics:**\n",
    "  - NMF results in a parts-based representation, making it suitable for tasks like image processing and topic modeling.\n",
    "\n",
    "- **Application:** NMF is commonly used for text mining, recommendation systems, and in various applications where interpretability is crucial (e.g., topic extraction from documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e11524-ced0-4063-8573-a501af2400a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Sample data\n",
    "data = np.random.rand(100, 50)  # 100 samples and 50 features\n",
    "print(\"Data Shape:\", data.shape)\n",
    "\n",
    "# Applying NMF\n",
    "nmf = NMF(n_components=10, init='random', random_state=0, max_iter=1000)\n",
    "nmf_reduced_data = nmf.fit_transform(data)\n",
    "\n",
    "print(\"NMF Reduced Data Shape:\", nmf_reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512537c-0f1c-4224-88d7-c6e63b60e62d",
   "metadata": {},
   "source": [
    "#### ***6.2.4.5. Independent Component Analysis (ICA)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be579d4-317f-4686-b25b-f1ef6c80b75f",
   "metadata": {},
   "source": [
    "Independent Component Analysis (ICA) is a computational technique used to separate a multivariate signal into additive, independent non-Gaussian components. It is widely used in signal processing.\n",
    "\n",
    "- **Characteristics:**\n",
    "  - ICA is effective in identifying hidden factors that underlie sets of random variables or observed data.\n",
    "\n",
    "- **Application:** ICA is particularly useful in applications such as image processing (e.g., separating mixed images), biomedical signal analysis (e.g., separating EEG signals), and financial data analysis.\n",
    "\n",
    "**Example: Using ICA with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723f5f4-9e31-4e01-a95b-4c58e35fb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "# Sample data\n",
    "data = np.random.rand(100, 5)  # 100 samples and 5 features\n",
    "print(\"Data Shape:\", data.shape)\n",
    "\n",
    "# Applying ICA\n",
    "ica = FastICA(n_components=3)\n",
    "ica_reduced_data = ica.fit_transform(data)\n",
    "\n",
    "print(\"ICA Reduced Data Shape:\", ica_reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13646b5-1a9b-4e07-944c-adba281ff724",
   "metadata": {},
   "source": [
    "#### ***6.2.4.6. Variance Thresholding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd758c-150b-4ed3-92d3-0631c067623e",
   "metadata": {},
   "source": [
    "Variance Thresholding is a simple feature selection method that will remove all features whose variance does not meet a certain threshold. This technique is particularly useful for removing features with low variance which are unlikely to contribute meaningful information to the model.\n",
    "\n",
    "- **Characteristics:**\n",
    "  - It requires setting a threshold value for variance; features with variance below this threshold are removed.\n",
    "  \n",
    "- **Application:** Variance Thresholding is commonly used in preprocessing steps to reduce the dimensionality of datasets by eliminating uninformative features.\n",
    "\n",
    "**Example: Using VarianceThreshold with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122b770-1861-475a-9912-534b01035893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[0, 0, 11, 9],\n",
    "                 [1, 0, 8, 4],\n",
    "                 [1, 0, 3, 4],\n",
    "                 [1, 0, 8, 9]])\n",
    "print(\"Data Shape:\", data.shape)\n",
    "\n",
    "# Applying Variance Threshold\n",
    "threshold = 0.1\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "reduced_data = selector.fit_transform(data)\n",
    "\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Reduced Data Shape:\", reduced_data.shape)\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af716693-a96e-4779-a9b3-1666d2de05c9",
   "metadata": {},
   "source": [
    "#### ***6.2.4.7. Filter Methods***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f28195-c126-44e6-b953-d9d055bf2b8a",
   "metadata": {},
   "source": [
    "Filter methods are a type of feature selection technique that relies on the intrinsic properties of the data to select the dimensions or features. These methods evaluate the relevance of features independently from the learning algorithm.\n",
    "\n",
    "- **Characteristics:**\n",
    "  - Filter methods usually use statistical measures to select the best features (e.g., correlation coefficient, chi-square test, mutual information).\n",
    "\n",
    "- **Application:** They are often used as a preprocessing step to improve the performance of supervised learning algorithms or to reduce computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cb257-f1b3-4519-8d6e-f069d1e0f44b",
   "metadata": {},
   "source": [
    "**Example: Using SelectKBest with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa0fb4-ecfe-42ab-8fe2-fb103da42faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "x, y = data.data, data.target\n",
    "print(\"Data Shape:\", x.shape)\n",
    "\n",
    "# Applying Filter Method\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "x_reduced = selector.fit_transform(x, y)\n",
    "\n",
    "print(\"Filtered Data Shape:\", x_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83827ead-b0b6-42b9-acb9-dd3936b3c9bc",
   "metadata": {},
   "source": [
    "<p style=\"background-color: lightgreen; text-align: center; font-size: 18px; color: red; padding: 5px; border-radius: 10px;\"><b>Exercise 3</b></p>\n",
    "\n",
    "1. **Load the Dataset:** Start by loading the breast cancer dataset from the `BreastCancer.csv` file using pandas.\n",
    "\n",
    "2. **Scale the Data:** Apply min-max scaling for all the input features.\n",
    "\n",
    "3. **Choose and Apply a Dimensionality Reduction Technique:** Select one method to reduce the number of input features. Transform your feature set using the selected technique, ensuring you keep a manageable number of components or features suitable for regression analysis.\n",
    "\n",
    "5. **Prepare the Target Variable:** Identify and separate the target variable from the feature set. If the dataset includes a class label (e.g., benign or malignant), use appropriate numerical coding or transformation.\n",
    "\n",
    "6. **Split the Data:** Use a part of your dataset for training and another part for testing (e.g., 80% training, 20% testing).\n",
    "\n",
    "7. **Build a Linear Regression Model:** Using `sklearn.linear_model.LinearRegression`, fit a linear regression model on the reduced feature set.\n",
    "\n",
    "8. **Evaluate the Model:** Assess the model's performance using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9b4ee-af2b-4114-8d3e-40d691f105b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9c164-58a3-4637-852f-94a9f4855568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484d648-a50a-4c3c-83f3-45c1890ad198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4a6be-5915-407c-90fb-1773aed3207b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d60e4-ef59-496f-8b65-f111cc42820b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c10ea-cd1e-41c2-800a-c0e2d49f02e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd6bde-ed74-4775-b9b5-342aa30eec13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

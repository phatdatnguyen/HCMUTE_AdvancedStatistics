{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2d6e30",
   "metadata": {},
   "source": [
    "# **Chapter 5. Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4301ad-93fc-4ab2-bd36-e0e18554a40c",
   "metadata": {},
   "source": [
    "## **5.1. Definition of Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579219b-fb19-4fc3-b7f9-ebf14ff8fa02",
   "metadata": {},
   "source": [
    "Optimization refers to the process of finding the maximum or minimum of a function by adjusting its variables. Specifically, in the field of machine learning, the objective is often to minimize a cost function, which quantifies the difference between the modelâ€™s predictions and the actual outcomes. Optimization is a fundamental aspect of various fields, including mathematics, engineering, economics, and statistics. It involves selecting the best option among a set of alternatives, often under constraints. In the context of machine learning and statistical modeling, optimization algorithms are crucial for training models and improving their performance.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Objective Function**: The function that needs to be optimized (minimized or maximized).\n",
    "- **Variables**: The parameters that can be adjusted to achieve the optimization.\n",
    "- **Constraints**: Conditions that the solution must satisfy. These can be equality or inequality restrictions.\n",
    "\n",
    "**Applications of Optimization**\n",
    "\n",
    "Optimization is widely applied across various domains:\n",
    "\n",
    "- **Machine Learning**: Training models by minimizing loss functions to improve accuracy.\n",
    "- **Operations Research**: Resource allocation problems, supply chain optimization, scheduling, and logistics.\n",
    "- **Finance**: Portfolio optimization to maximize returns while minimizing risk.\n",
    "- **Engineering**: Design optimization to improve performance while adhering to constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a304a2-f0a1-47c6-be6c-21cbb3ac5835",
   "metadata": {},
   "source": [
    "## **5.2. Types of Optimization Problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf9325-e9b5-461f-a20d-adb342da4794",
   "metadata": {},
   "source": [
    "Optimization problems can be broadly categorized into several types:\n",
    "\n",
    "1. **Linear vs. Non-linear Optimization**:\n",
    "   - **Linear Optimization**: Involves an objective function and constraints that are linear functions of the decision variables.\n",
    "   - **Non-linear Optimization**: Involves at least one non-linear function in the objective or constraints.\n",
    "\n",
    "2. **Convex vs. Non-convex Optimization**:\n",
    "   - **Convex Optimization**: The objective function is convex, meaning that any local minimum is a global minimum.\n",
    "   - **Non-convex Optimization**: The objective function may have multiple local minima, which complicates the optimization process.\n",
    "\n",
    "3. **Constrained vs. Unconstrained Optimization**:\n",
    "   - **Constrained Optimization**: The optimization problem includes constraints that limit the feasible solutions.\n",
    "   - **Unconstrained Optimization**: There are no restrictions on the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcd878-84ad-4ffa-ad01-624806074a45",
   "metadata": {},
   "source": [
    "## **5.3. Optimization Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9de5f-de80-4d92-bd81-164ca4e77c53",
   "metadata": {},
   "source": [
    "### **5.3.1. Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaef992-3d9e-4e0b-a20a-9316c4d64c18",
   "metadata": {},
   "source": [
    "Gradient descent is one of the most widely used optimization algorithms in machine learning. It iteratively adjusts the model parameters in the direction of the steepest decrease of the objective function.\n",
    "\n",
    "**Key Steps**:\n",
    "1. Initialize parameters randomly.\n",
    "2. Compute the gradient (i.e., the partial derivatives) of the objective function.\n",
    "3. Update parameters: \n",
    "   $$ \\theta = \\theta - \\alpha \\nabla J(\\theta) $$\n",
    "   where $\\alpha$ is the learning rate and $\\nabla J(\\theta)$ is the gradient at $\\theta$.\n",
    "\n",
    "**Types of Gradient Descent**:\n",
    "- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses one training example at a time to update the parameters, resulting in faster convergence.\n",
    "- **Mini-batch Gradient Descent**: A compromise that uses a small batch of data to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b34b69-ff26-4a40-b80e-c18d65bda27b",
   "metadata": {},
   "source": [
    "**Gradient Descent Example**\n",
    "\n",
    "Below is a simplified example of using Gradient Descent to find the minimum of a simple quadratic function. Let's say we want to find the minimum value of the function:\n",
    "\n",
    "$$ f(x) = (x - 3)^2 + 2 $$\n",
    "\n",
    "The minimum of this function occurs at $x = 3$.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   We define the function $f(x)$ and its derivative $f'(x)$ (the gradient). The derivative will help us understand the direction to move in order to minimize the function.\n",
    "\n",
    "2. **Implementing Gradient Descent**:\n",
    "   We'll start with an initial guess and iteratively adjust our value of $x$ using the gradient.\n",
    "\n",
    "3. **Parameters**:\n",
    "   We define a learning rate to control how much we adjust $x$ at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230c812-d552-45c8-8471-4006dba13205",
   "metadata": {},
   "source": [
    "**Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7644e4-c50d-43e7-98b7-a3c2e8dcc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06cb61-e738-4ad7-a216-28750e79c8de",
   "metadata": {},
   "source": [
    "**Step 1: Define the Function and Its Derivative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87ce82-9d77-4105-b3d0-6758a6676bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its derivative\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2 * (x - 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9ea59-4e4c-452c-9d3b-3e5c0c85c8d6",
   "metadata": {},
   "source": [
    "**Step 2: Perform Optimization with Gradient Descent Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8302125-a279-471a-afd6-cffe7bfbc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "def gradient_descent(starting_point, learning_rate, n_iterations, tolerance):\n",
    "    x = starting_point\n",
    "    x_history = [x]  # Store the history for plotting\n",
    "    for _ in range(n_iterations):\n",
    "        # Update x using the gradient\n",
    "        x = x - learning_rate * f_prime(x)\n",
    "        x_history.append(x)\n",
    "        if abs(x - x_history[-2]) < tolerance:\n",
    "            break\n",
    "    return x, x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff8f9c-b62b-418c-a967-39ff8357b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "starting_point = 0.0  # Initial guess\n",
    "learning_rate = 0.1   # Learning rate\n",
    "n_iterations = 100    # Number of iterations\n",
    "tolerance = 0.0001\n",
    "\n",
    "# Perform Gradient Descent\n",
    "minimum, history = gradient_descent(starting_point, learning_rate, n_iterations, tolerance)\n",
    "\n",
    "# Print the result\n",
    "print(f'Number of iterations: {len(history)}')\n",
    "print(f'The estimated minimum occurs at x = {minimum}')\n",
    "print(f'The minimum value of the function is f(x) = {f(minimum)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02254d0-3c6e-4b39-8373-c211c1a11709",
   "metadata": {},
   "source": [
    "**Step 3: Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945662f-a2e1-42fe-9888-c2ac7b403f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the function and the path taken by gradient descent\n",
    "x_values = np.linspace(-1, 7, 100)\n",
    "y_values = f(x_values)\n",
    "\n",
    "plt.plot(x_values, y_values, label='$f(x) = (x - 3)^2 + 2$')\n",
    "plt.scatter(history, f(np.array(history)), color='red', label='Gradient Descent Steps', zorder=5)\n",
    "plt.title('Gradient Descent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220a536-9308-4b49-9afa-82cd60f21f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure for animation\n",
    "x_values = np.linspace(-1, 7, 100)\n",
    "y_values = f(x_values)\n",
    "\n",
    "# Animation loop\n",
    "for frame in range(len(history)):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_values, y_values, label='f(x) = (x - 3)^2 + 2')\n",
    "    \n",
    "    # Current x position\n",
    "    x_current = history[frame]\n",
    "    y_current = f(x_current)\n",
    "    \n",
    "    # Current point\n",
    "    plt.plot(x_current, y_current, 'ro')  # Current x point\n",
    "    plt.text(x_current, y_current, f'  ({x_current:.2f}, {y_current:.2f})', fontsize=12, verticalalignment='bottom')\n",
    "\n",
    "    # Calculate slope and tangent line\n",
    "    slope = f_prime(x_current)\n",
    "    tangent_x = np.linspace(x_current - 1, x_current + 1, 100)\n",
    "    tangent_y = slope * (tangent_x - x_current) + y_current\n",
    "    plt.plot(tangent_x, tangent_y, 'g--', label='Tangent Line')\n",
    "\n",
    "    plt.xlim([-1, 7])\n",
    "    plt.ylim([0, 10])\n",
    "    plt.title(f'Iteration: {frame + 1}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # Display the updated plot\n",
    "    display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    plt.close()\n",
    "    \n",
    "    # Pause for a moment to allow the user to see the plot\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5adb032-67aa-443f-bf0a-a63193339bfb",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Function Definition**: We define the function $f(x)$ and its derivative $f'(x)$.\n",
    "- **Gradient Descent**: Starting from an initial guess (in this case, $x = 0$), we compute the derivative and update $x$ in the opposite direction of the gradient using a specified learning rate.\n",
    "- **Iterations**: By repeatedly adjusting $x$ using the gradient, we approach the minimum of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c0b67-6524-4f52-999d-0df93ca8fea1",
   "metadata": {},
   "source": [
    "<p style=\"background-color: lightgreen; text-align: center; font-size: 18px; color: red; padding: 5px; border-radius: 10px;\"><b>Exercise 1</b></p>\n",
    "\n",
    "Use gradient descent to find the minimum of the following function:\n",
    "\n",
    "$$ f(x) = x^4 - 3x^3 + 2 $$\n",
    "\n",
    "This function has a global minimum and also has a local maximum, making it a good candidate for studying gradient descent behavior.\n",
    "\n",
    "1. **Define the Function**: Write a function that computes $f(x)$ and its derivative $f'(x)$.\n",
    "\n",
    "2. **Gradient Descent Implementation**: Implement the gradient descent algorithm. Start at an initial guess of your choice (for example, $x = -1$) and set the learning rate to a reasonable value (try $\\alpha = 0.01$).\n",
    "\n",
    "3. **Plotting the Function**: Use Matplotlib to visualize the function and the steps taken by gradient descent. Show the current point and the tangent line at each iteration.\n",
    "\n",
    "4. **Animating the Process**: Animate the process similarly to the previous example using a display loop to demonstrate how gradient descent progresses.\n",
    "\n",
    "5. **Analyze the Results**: After running your gradient descent algorithm, analyze the output to find where the minimum occurred and the value of the function at that point. Discuss whether the chosen initial point influenced the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62e395-690f-40cf-ba51-cda13b117ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4efad-fed1-4293-9c4b-cf496f51435f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e876c-9bb2-44aa-ae22-935a577711c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03dbd5-1238-40c5-94c9-e23e07876f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ad41e-cf78-43ed-a10c-fab648ea64b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77019118-6283-444c-82dc-1015c2a84078",
   "metadata": {},
   "source": [
    "### **5.3.2. Newton's Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165347c0-5e7d-430c-8a5e-686df709a81a",
   "metadata": {},
   "source": [
    "Newton's method is an iterative optimization technique that uses second-order derivatives (the Hessian matrix) to find the critical points of a function.\n",
    "\n",
    "**Update Rule**:\n",
    "$$ \\theta = \\theta - H^{-1} \\nabla J(\\theta) $$\n",
    "where $H$ is the Hessian matrix, and $\\nabla J(\\theta)$ is the gradient.\n",
    "\n",
    "Newton's method converges faster than gradient descent, but it can be computationally expensive for high-dimensional problems due to the calculation of the Hessian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cfacd-0061-4f9f-bb4e-1493396dac58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Newton's Method Example**\n",
    "\n",
    "Below is an example of using Newton's Method to find the minimum of a function. Newton's Method is an iterative optimization technique that uses both the first and second derivatives to find the critical points of a function.\n",
    "\n",
    "We will find the minimum of the following function:\n",
    "\n",
    "$$ f(x) = x^3 - 6x^2 + 9x + 3 $$\n",
    "\n",
    "This function has a local minimum and a global minimum, allowing us to demonstrate Newtonâ€™s Method effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a98b9-a210-4231-b14d-3be5300e24a5",
   "metadata": {},
   "source": [
    "**Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0683d-6920-4626-b613-9fb142f6862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c73bb9-a85c-4909-a67d-66c6157b2543",
   "metadata": {},
   "source": [
    "**Step 1: Define the Function and Its Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f29fe6-7a2a-492a-9abe-698e8d24ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its derivatives\n",
    "def f(x):\n",
    "    return x**3 - 6*x**2 + 9*x + 3\n",
    "\n",
    "def f_prime(x):\n",
    "    return 3*x**2 - 12*x + 9\n",
    "\n",
    "def f_double_prime(x):\n",
    "    return 6*x - 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da05039-6387-4ef2-abac-d0ba745dff96",
   "metadata": {},
   "source": [
    "**Step 2: Implement Newton's Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50b865-f9ea-480c-a635-a0f7389179b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method Algorithm\n",
    "def newtons_method(starting_point, tolerance, max_iterations):\n",
    "    x = starting_point\n",
    "    history = [x]\n",
    "    for _ in range(max_iterations):\n",
    "        # Calculate the first and second derivative\n",
    "        f_prime_val = f_prime(x)\n",
    "        f_double_prime_val = f_double_prime(x)\n",
    "\n",
    "        # Check if the second derivative is zero to avoid division by zero\n",
    "        if f_double_prime_val == 0:\n",
    "            print(\"Second derivative is zero. Newton's method fails here.\")\n",
    "            break\n",
    "\n",
    "        # Update x using the Newton's Method formula\n",
    "        x_new = x - f_prime_val / f_double_prime_val\n",
    "        history.append(x_new)\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(x_new - x) < tolerance:\n",
    "            break\n",
    "        x = x_new\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1eb29-c705-4db1-8a06-a7471d7a3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "starting_point = 2.5  # Initial guess\n",
    "tolerance = 1e-8      # Convergence tolerance\n",
    "max_iterations = 100  # Maximum number of iterations\n",
    "\n",
    "# Perform Newton's Method\n",
    "history = newtons_method(starting_point, tolerance, max_iterations)\n",
    "print(f'Number of iterations: {len(history)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ed978-af30-41e0-aa4f-d6dbdaeee149",
   "metadata": {},
   "source": [
    "**Step 3: Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b2ed8-5676-4de9-b313-a8f3f417b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure for animation\n",
    "x_values = np.linspace(-1, 5, 100)\n",
    "y_values = f(x_values)\n",
    "\n",
    "# Animation loop\n",
    "for i in range(len(history)):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_values, y_values, label='$f(x) = x^3 - 6x^2 + 9x + 3$')\n",
    "\n",
    "    # Current x position\n",
    "    x_current = history[i]\n",
    "    y_current = f(x_current)\n",
    "\n",
    "    # Current point\n",
    "    plt.plot(x_current, y_current, 'ro')  # Current x point\n",
    "    plt.text(x_current, y_current, f'  ({x_current:.2f}, {y_current:.2f})', fontsize=12, verticalalignment='bottom')\n",
    "\n",
    "    # Calculate slope and tangent line\n",
    "    slope = f_prime(x_current)\n",
    "    tangent_x = np.linspace(x_current - 1, x_current + 1, 100)\n",
    "    tangent_y = slope * (tangent_x - x_current) + y_current\n",
    "    plt.plot(tangent_x, tangent_y, 'g--', label='Tangent Line')\n",
    "\n",
    "    plt.xlim([-1, 5])\n",
    "    plt.ylim([-5, 10])\n",
    "    plt.title(f'Iteration: {i + 1}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Display the updated plot\n",
    "    display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Pause for a moment to allow the user to see the plot\n",
    "    time.sleep(0.1)\n",
    "\n",
    "plt.close()  # Close the last figure after animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6cd06-3fba-41f0-a35b-16498fc4f68d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Function Definition**: We define the function $f(x)$ along with its first and second derivatives.\n",
    "- **Newtonâ€™s Method**: The `newtons_method` function iteratively updates the estimate for the minimum based on the derivatives. It checks for convergence based on a specified tolerance.\n",
    "- **Animation Loop**: The loop visualizes the current position during each iteration, showing the point and the tangent line at that position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b35de-626c-42d1-9915-503927ae71df",
   "metadata": {},
   "source": [
    "<p style=\"background-color: lightgreen; text-align: center; font-size: 18px; color: red; padding: 5px; border-radius: 10px;\"><b>Exercise 2</b></p>\n",
    "\n",
    "Use Newton's Method to find the minimum of the following function:\n",
    "\n",
    "$$ f(x) = x^4 - 8x^3 + 18x^2 - 5 $$\n",
    "\n",
    "1. Define the function $f(x)$ and calculate its first and second derivatives:\n",
    "   - First Derivative: $f'(x)$\n",
    "   - Second Derivative: $f''(x)$\n",
    "\n",
    "2. Implement Newton's Method to find the minimum starting from an initial guess of your choice (for example, $x = 0$).\n",
    "\n",
    "3. Plot the function $f(x)$ and animate the steps taken by Newton's Method, highlighting the current point and the tangent line at each iteration.\n",
    "\n",
    "4. Analyze your results:\n",
    "   - Where did Newton's Method converge?\n",
    "   - What is the value of the function at that point?\n",
    "   - Did the choice of the initial point affect the convergence? If so, how?\n",
    "\n",
    "5. (Optional) Test different initial guesses and observe how they affect the convergence behavior of Newton's Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3e2ca-fe12-4e6a-abfa-c80a13e0ceb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf207e-662e-45e6-87f9-079f6a2f0026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca245a9b-e19b-4211-9d91-09df2352a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ccb3-88c6-49b4-b3bb-3169e96e1353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e411df-1755-4176-a3aa-92a154024564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8da50df-a74a-4962-a523-cf0528895451",
   "metadata": {},
   "source": [
    "### **5.3.3. Other Optimization Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85431f-3378-413d-b86e-8ec46b0385b9",
   "metadata": {},
   "source": [
    "- **Simulated Annealing**: A probabilistic technique that explores the solution space and jumps out of local minima by accepting worse solutions with a certain probability.\n",
    "- **Genetic Algorithms**: A search heuristic inspired by natural selection that mimics the process of evolution to optimize solutions.\n",
    "- **Conjugate Gradient Method**: An iterative method for solving large systems of linear equations, particularly for optimization problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
